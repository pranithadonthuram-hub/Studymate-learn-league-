# -*- coding: utf-8 -*-
"""granite-3.2-2b-instruct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/ibm-granite/granite-3.2-2b-instruct.ipynb
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/ibm-granite/granite-3.2-2b-instruct

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.2-2b-instruct)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""



# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.2-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.2-2b-instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

!pip install transformers accelerate bitsandbytes gradio PyPDF2

import gradio as gr
import PyPDF2
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model
model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    device_map="auto",
    torch_dtype=torch.float16
)

# Extract text from PDF
def extract_text_from_pdf(pdf):
    reader = PyPDF2.PdfReader(pdf.name)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text

# QA function
def answer_question(pdf, question):
    if pdf is None:
        return "Upload a PDF first."

    if question.strip() == "":
        return "Enter a question."

    pdf_text = extract_text_from_pdf(pdf)
    pdf_text = pdf_text[:2000]   # limit to avoid token overflow

    prompt = (
        "You are an AI assistant. Answer only from the PDF.\n\n"
        f"PDF Text:\n{pdf_text}\n\n"
        f"Question: {question}\nAnswer:"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=150)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # remove prompt part
    if "Answer:" in response:
        response = response.split("Answer:")[-1].strip()

    return response

# Gradio UI
with gr.Blocks() as app:
    gr.Markdown("# üìò StudyMate: AI PDF Question Answering")

    pdf_input = gr.File(label="Upload PDF")
    question_input = gr.Textbox(label="Ask your question")
    answer_output = gr.Textbox(label="Answer")

    btn = gr.Button("Get Answer")
    btn.click(answer_question, [pdf_input, question_input], answer_output)

app.launch(share=True)